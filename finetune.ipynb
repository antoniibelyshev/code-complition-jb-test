{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/rolos-ml-p39/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PhiForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/phi-1_5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = PhiForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory reserved by PyTorch: 5.296875 GB\n"
     ]
    }
   ],
   "source": [
    "# Get the memory reserved by PyTorch\n",
    "reserved_memory = torch.cuda.memory_reserved()\n",
    "reserved_memory_mb = reserved_memory / 1024**3  # Convert bytes to megabytes\n",
    "\n",
    "print(\"Memory reserved by PyTorch:\", reserved_memory_mb, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for kotlin files...\n",
      "Parsing functions in kotlin files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:03<00:00, 78.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse errors count: 62, declaration errors count: 0\n",
      "total number of samples: 604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kotlin_code = utils.extract_kotlin_code()\n",
    "train_kotlin_prompts, test_kotlin_prompts, train_kotlin_answers, test_kotlin_answers = train_test_split(*kotlin_code, test_size=0.1)\n",
    "\n",
    "train_dataset = utils.CodeCompletionDataset(train_kotlin_prompts, train_kotlin_answers, train=True)\n",
    "\n",
    "test_kotlin_dataset = utils.CodeCompletionDataset(test_kotlin_prompts, test_kotlin_answers, train=False)\n",
    "\n",
    "test_codexglue_dataset = utils.CodeCompletionDataset(*utils.read_codexglue_test_data(n=1000), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory reserved by PyTorch: 5.296875 GB\n"
     ]
    }
   ],
   "source": [
    "# Get the memory reserved by PyTorch\n",
    "reserved_memory = torch.cuda.memory_reserved()\n",
    "reserved_memory_mb = reserved_memory / 1024**3  # Convert bytes to megabytes\n",
    "\n",
    "print(\"Memory reserved by PyTorch:\", reserved_memory_mb, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:04<24:46,  1.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/coder/project/finetune.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/finetune.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39;49mevaluate(model, tokenizer, test_codexglue_dataset, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/finetune.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/finetune.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m prompt, answer \u001b[39m=\u001b[39m test_codexglue_dataset[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/project/utils/evaluate.py:34\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, tokenizer, eval_dataset, min_new_tokens, max_new_tokens)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mfor\u001b[39;00m prompt, answer \u001b[39min\u001b[39;00m tqdm(eval_dataset):\n\u001b[1;32m     33\u001b[0m         \u001b[39mif\u001b[39;00m answer \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m             completions\u001b[39m.\u001b[39mappend(sample(model, tokenizer, prompt, min_new_tokens\u001b[39m=\u001b[39;49mmin_new_tokens, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens))\n\u001b[1;32m     35\u001b[0m             answers\u001b[39m.\u001b[39mappend(answer)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     38\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maccuracy score\u001b[39m\u001b[39m\"\u001b[39m: accuracy_score(answers, completions),\n\u001b[1;32m     39\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbleu score\u001b[39m\u001b[39m\"\u001b[39m: corpus_bleu([[ans\u001b[39m.\u001b[39msplit()] \u001b[39mfor\u001b[39;00m ans \u001b[39min\u001b[39;00m answers], [comp\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m comp \u001b[39min\u001b[39;00m completions]),\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrouge\u001b[39m\u001b[39m\"\u001b[39m: Rouge()\u001b[39m.\u001b[39mget_scores(completions, answers, avg\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m\"\u001b[39m\u001b[39mrouge-1\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     41\u001b[0m }\n",
      "File \u001b[0;32m~/project/utils/evaluate.py:11\u001b[0m, in \u001b[0;36msample\u001b[0;34m(model, tokenizer, prompt, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(model: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, tokenizer, prompt: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     10\u001b[0m     prompt_tensor \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 11\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     12\u001b[0m         prompt_tensor,\n\u001b[1;32m     13\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     15\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     16\u001b[0m         bos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     17\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     completion \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m][prompt_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m     \u001b[39mdel\u001b[39;00m prompt_tensor, output\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[39m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_greedy_search(\n\u001b[1;32m   1577\u001b[0m         input_ids,\n\u001b[1;32m   1578\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1579\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1580\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1581\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1582\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1583\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1584\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1585\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1586\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1587\u001b[0m     )\n\u001b[1;32m   1589\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2495\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2496\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2497\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2498\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2499\u001b[0m )\n\u001b[1;32m   2501\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:1169\u001b[0m, in \u001b[0;36mPhiForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1166\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1168\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1170\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1171\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1172\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1173\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1174\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1175\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1176\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1177\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1178\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[1;32m   1181\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1182\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:1048\u001b[0m, in \u001b[0;36mPhiModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1040\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1041\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         output_attentions,\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1047\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1048\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1049\u001b[0m         hidden_states,\n\u001b[1;32m   1050\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1051\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1052\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1053\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1054\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1055\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:779\u001b[0m, in \u001b[0;36mPhiDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    776\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    778\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    780\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    781\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    782\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    783\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    784\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    785\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    786\u001b[0m )\n\u001b[1;32m    787\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(attn_outputs)\n\u001b[1;32m    789\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:350\u001b[0m, in \u001b[0;36mPhiAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    345\u001b[0m key_rot, key_pass \u001b[39m=\u001b[39m (\n\u001b[1;32m    346\u001b[0m     key_states[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb\u001b[39m.\u001b[39mdim],\n\u001b[1;32m    347\u001b[0m     key_states[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb\u001b[39m.\u001b[39mdim :],\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m \u001b[39m# [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m query_rot, key_rot \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n\u001b[1;32m    352\u001b[0m \u001b[39m# [batch_size, seq_length, num_heads, head_dim]\u001b[39;00m\n\u001b[1;32m    353\u001b[0m query_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((query_rot, query_pass), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:197\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Applies Rotary Position Embedding to the query and key tensors.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m    `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m cos \u001b[39m=\u001b[39m cos[position_ids]\u001b[39m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 197\u001b[0m sin \u001b[39m=\u001b[39m sin[position_ids]\u001b[39m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    198\u001b[0m q_embed \u001b[39m=\u001b[39m (q \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_half(q) \u001b[39m*\u001b[39m sin)\n\u001b[1;32m    199\u001b[0m k_embed \u001b[39m=\u001b[39m (k \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_half(k) \u001b[39m*\u001b[39m sin)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key, value in utils.evaluate(model, tokenizer, test_codexglue_dataset, max_new_tokens=20).items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "prompt, answer = test_codexglue_dataset[0]\n",
    "print(f\"\\nexample prompt: {prompt}\\n\")\n",
    "print(f\"example completion: {utils.sample(model, tokenizer, prompt, min_new_tokens=2, max_new_tokens=20)}\\n\")\n",
    "print(f\"example true answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 23/61 [00:13<00:22,  1.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/coder/project/finetune.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/finetune.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39;49mevaluate(model, tokenizer, test_kotlin_dataset, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/finetune.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/finetune.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m prompt, answer \u001b[39m=\u001b[39m test_kotlin_dataset[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/project/utils/evaluate.py:34\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, tokenizer, eval_dataset, min_new_tokens, max_new_tokens)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mfor\u001b[39;00m prompt, answer \u001b[39min\u001b[39;00m tqdm(eval_dataset):\n\u001b[1;32m     33\u001b[0m         \u001b[39mif\u001b[39;00m answer \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m             completions\u001b[39m.\u001b[39mappend(sample(model, tokenizer, prompt, min_new_tokens\u001b[39m=\u001b[39;49mmin_new_tokens, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens))\n\u001b[1;32m     35\u001b[0m             answers\u001b[39m.\u001b[39mappend(answer)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     38\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maccuracy score\u001b[39m\u001b[39m\"\u001b[39m: accuracy_score(answers, completions),\n\u001b[1;32m     39\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbleu score\u001b[39m\u001b[39m\"\u001b[39m: corpus_bleu([[ans\u001b[39m.\u001b[39msplit()] \u001b[39mfor\u001b[39;00m ans \u001b[39min\u001b[39;00m answers], [comp\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m comp \u001b[39min\u001b[39;00m completions]),\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrouge\u001b[39m\u001b[39m\"\u001b[39m: Rouge()\u001b[39m.\u001b[39mget_scores(completions, answers, avg\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m\"\u001b[39m\u001b[39mrouge-1\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     41\u001b[0m }\n",
      "File \u001b[0;32m~/project/utils/evaluate.py:11\u001b[0m, in \u001b[0;36msample\u001b[0;34m(model, tokenizer, prompt, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(model: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, tokenizer, prompt: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     10\u001b[0m     prompt_tensor \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 11\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     12\u001b[0m         prompt_tensor,\n\u001b[1;32m     13\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     15\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     16\u001b[0m         bos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     17\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     completion \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m][prompt_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m     \u001b[39mdel\u001b[39;00m prompt_tensor, output\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[39m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_greedy_search(\n\u001b[1;32m   1577\u001b[0m         input_ids,\n\u001b[1;32m   1578\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1579\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1580\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1581\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1582\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1583\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1584\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1585\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1586\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1587\u001b[0m     )\n\u001b[1;32m   1589\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2495\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2496\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2497\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2498\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2499\u001b[0m )\n\u001b[1;32m   2501\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:1169\u001b[0m, in \u001b[0;36mPhiForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1166\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1168\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1170\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1171\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1172\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1173\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1174\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1175\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1176\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1177\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1178\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[1;32m   1181\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1182\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:1048\u001b[0m, in \u001b[0;36mPhiModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1040\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1041\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         output_attentions,\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1047\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1048\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1049\u001b[0m         hidden_states,\n\u001b[1;32m   1050\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1051\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1052\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1053\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1054\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1055\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:779\u001b[0m, in \u001b[0;36mPhiDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    776\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    778\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    780\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    781\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    782\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    783\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    784\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    785\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    786\u001b[0m )\n\u001b[1;32m    787\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(attn_outputs)\n\u001b[1;32m    789\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:338\u001b[0m, in \u001b[0;36mPhiAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe cache structure has changed since version v4.36. If you are using \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfor auto-regressive decoding with k/v caching, please make sure to initialize the attention class \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith a layer index.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m         )\n\u001b[1;32m    337\u001b[0m     kv_seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m past_key_value\u001b[39m.\u001b[39mget_usable_length(kv_seq_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_idx)\n\u001b[0;32m--> 338\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39;49mkv_seq_len)\n\u001b[1;32m    340\u001b[0m \u001b[39m# Partial rotary embedding\u001b[39;00m\n\u001b[1;32m    341\u001b[0m query_rot, query_pass \u001b[39m=\u001b[39m (\n\u001b[1;32m    342\u001b[0m     query_states[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb\u001b[39m.\u001b[39mdim],\n\u001b[1;32m    343\u001b[0m     query_states[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb\u001b[39m.\u001b[39mdim :],\n\u001b[1;32m    344\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:114\u001b[0m, in \u001b[0;36mPhiRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m seq_len \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_len_cached:\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_cos_sin_cache(seq_len\u001b[39m=\u001b[39mseq_len, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice, dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcos_cached[:seq_len]\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype),\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin_cached[:seq_len]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype),\n\u001b[1;32m    116\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key, value in utils.evaluate(model, tokenizer, test_kotlin_dataset, max_new_tokens=20).items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "prompt, answer = test_kotlin_dataset[0]\n",
    "print(f\"\\nexample prompt: {prompt}\\n\")\n",
    "print(f\"example completion: {utils.sample(model, tokenizer, prompt, min_new_tokens=2, max_new_tokens=20)}\\n\")\n",
    "print(f\"example true answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mantonii-belyshev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20240504_151840-s0lj55l3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/finetune/runs/s0lj55l3' target=\"_blank\">star-womprat-22</a></strong> to <a href='https://wandb.ai/antonii-belyshev/finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/finetune' target=\"_blank\">https://wandb.ai/antonii-belyshev/finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/finetune/runs/s0lj55l3' target=\"_blank\">https://wandb.ai/antonii-belyshev/finetune/runs/s0lj55l3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7885:   9%|▉         | 25/272 [00:04<00:42,  5.77it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 39.59 GiB total capacity; 10.26 GiB already allocated; 44.88 MiB free; 10.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/coder/project/finetune.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://research.constructor.tech/home/coder/project/finetune.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mtrain_model(model, tokenizer, train_dataset)\n",
      "File \u001b[0;32m~/project/utils/train.py:44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, train_dataset, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdel\u001b[39;00m tokens\n\u001b[1;32m     43\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 44\u001b[0m loss \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49minput_ids)\u001b[39m.\u001b[39mloss\n\u001b[1;32m     45\u001b[0m \u001b[39mdel\u001b[39;00m input_ids, attention_mask\n\u001b[1;32m     47\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/rolos-ml-p39/site-packages/transformers/models/phi/modeling_phi.py:1196\u001b[0m, in \u001b[0;36mPhiForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[39m# Enable model parallelism\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     shift_labels \u001b[39m=\u001b[39m shift_labels\u001b[39m.\u001b[39mto(shift_logits\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1196\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(shift_logits, shift_labels)\n\u001b[1;32m   1198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1199\u001b[0m     output \u001b[39m=\u001b[39m (logits,) \u001b[39m+\u001b[39m outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 39.59 GiB total capacity; 10.26 GiB already allocated; 44.88 MiB free; 10.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = utils.train_model(model, tokenizer, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory reserved by PyTorch: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# Get the memory reserved by PyTorch\n",
    "reserved_memory = torch.cuda.memory_reserved()\n",
    "reserved_memory_mb = reserved_memory / 1024**3  # Convert bytes to megabytes\n",
    "\n",
    "print(\"Memory reserved by PyTorch:\", reserved_memory_mb, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  4 17:30:14 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   34C    P0    41W / 250W |  36862MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in utils.evaluate(model, tokenizer, test_codexglue_dataset, max_new_tokens=20).items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "prompt, answer = test_codexglue_dataset[0]\n",
    "print(f\"\\nexample prompt: {prompt}\\n\")\n",
    "print(f\"example completion: {utils.sample(model, tokenizer, prompt, min_new_tokens=2, max_new_tokens=20)}\\n\")\n",
    "print(f\"example true answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in utils.evaluate(model, tokenizer, test_kotlin_dataset, max_new_tokens=20).items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "prompt, answer = test_kotlin_dataset[0]\n",
    "print(f\"\\nexample prompt: {prompt}\\n\")\n",
    "print(f\"example completion: {utils.sample(model, tokenizer, prompt, min_new_tokens=2, max_new_tokens=20)}\\n\")\n",
    "print(f\"example true answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = PhiForCausalLM.from_pretrained(\"./fine_tuned_model\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompts, test_ground_truths = utils.read_codexglue_test_data()\n",
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# from rouge import Rouge\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# # Define function to calculate BLEU score\n",
    "# def calculate_bleu_score(predictions, references):\n",
    "#     return corpus_bleu([[ref.split()] for ref in references], [pred.split() for pred in predictions])\n",
    "\n",
    "# # Define function to calculate ROUGE score\n",
    "# def calculate_rouge_score(predictions, references):\n",
    "#     rouge = Rouge()\n",
    "#     scores = rouge.get_scores(predictions, references, avg=True)\n",
    "#     return scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f']\n",
    "\n",
    "# def generate_code_completions(prompts, max_length=100):\n",
    "#     generated_completions = []\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for prompt in tqdm(prompts):\n",
    "#             prompt_tensor = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#             output = model.generate(\n",
    "#                 prompt_tensor,\n",
    "#                 max_new_tokens=max_length,\n",
    "#                 num_return_sequences=1,\n",
    "#                 pad_token_id=model.config.pad_token_id,\n",
    "#                 eos_token_id=model.config.eos_token_id,\n",
    "#                 bos_token_id=model.config.bos_token_id,\n",
    "#             )\n",
    "#             completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#             generated_completions.append(completion)\n",
    "#     return generated_completions\n",
    "# generated_code_completions = generate_code_completions(test_ground_truths[:20])\n",
    "# # Calculate evaluation metrics\n",
    "# accuracy = sum(1 for pred, gt in zip(generated_code_completions, test_ground_truths) if pred == gt) / len(test_ground_truths[:20])\n",
    "# bleu_score = calculate_bleu_score(generated_code_completions, test_ground_truths[:20])\n",
    "# rouge_score_1, rouge_score_2, rouge_score_l = calculate_rouge_score(generated_code_completions, test_ground_truths[:20])\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"BLEU Score:\", bleu_score)\n",
    "# print(\"ROUGE Score (Rouge-1):\", rouge_score_1)\n",
    "# print(\"ROUGE Score (Rouge-2):\", rouge_score_2)\n",
    "# print(\"ROUGE Score (Rouge-L):\", rouge_score_l)\n",
    "\n",
    "model.save_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (user.isVerbose):\n",
      "        print(\"$DEBUG: $message\")\n",
      "\n",
      "\n",
      "if isSuccess(user):\n",
      "        message = \"[$HAS_SUCCESS] $user\"\n",
      "    elif isWarning(user):\n",
      "        message = \"[$HAS_WARNING] $user\"\n",
      "    elif isError(user):\n",
      "        message = \"[$HAS_ERROR] $user\"\n",
      "    else:\n",
      "        message = \"[$HAS_LOG] $user\"\n",
      "    user.l += message\n",
      "    if LOG!= \"console\n",
      "\n",
      "try:\n",
      "        user = self.user_lookup(request)\n",
      "        if user is not None:\n",
      "            response.processAll(user.receiveMessages())\n",
      "        else:\n",
      "            response.process(\"fail: user not found\")\n",
      "    except Exception:\n",
      "        response.process(\"fail: exception processing\")\n",
      "\n",
      "\n",
      "return {\n",
      "        table(\"user\", \"$name\", \"$auth\")\n",
      "        table(\"user\", \"$name\", \"$auth\", eager = eager)\n",
      "        table(\"user\", \"$name\", \"$auth\", \"caches\")\n",
      "    }[name]\n",
      "\n",
      "\n",
      "@wraps(this)\n",
      "    @require(methods) { \"require(methods.isNotEmpty() == true) { \" }\n",
      "    @require(it == typeOf<String>()) { \"Type of method is not string: $this\" }\n",
      "    require(it == typeOf<String?>()) { \"Type of method is not string: $this\" }\n",
      "    @require(it == typeOf<String?>()) { \"Type\n",
      "\n",
      "user = TwitterUser(user)\n",
      "    user.login()\n",
      "    return {\n",
      "        i: tweet for i, tweet in enumerate(user.search(user.followed_user.id, pages)!!)\n",
      "        if tweet is not null\n",
      "    }\n",
      "\n",
      "\n",
      "deformation_42 = []\n",
      "    for i in range(len(chn_names)):\n",
      "        deformation_42.append(data[i,:,:,2])\n",
      "    return deformation_42\n",
      "\n",
      "\n",
      "return self.get(LighterASTNode.Leaf)\n",
      "\n",
      "return self.get(label)\n",
      "\n",
      "\n",
      "return self.get(LighterASTNode.LeafNode)()\n",
      "\n",
      "\n",
      "return cls(name, length, descendants, **kw)\n",
      "\n",
      "\n",
      "return self.__newick\n",
      "}\n",
      "\n",
      "result = []\n",
      "    try:\n",
      "        result.append(Node.from(s, **kw))\n",
      "    except(NodeException) as e:\n",
      "        raise NodeException(\"Node is not a New YorkTree: $s\") from e\n",
      "    if strip_comments:\n",
      "        for i in range(len(result)):\n",
      "            result[i] = removeComments(result[i], stripComments = True)\n",
      "    return result\n",
      "\n",
      "\n",
      "if predicate is None:\n",
      "        predicate = lambda node: True\n",
      "    for node in self.walk(predicate, **kw)\n",
      "    return visitor(node)\n",
      "\n",
      "\n",
      "if strict:\n",
      "        # Only ASCII characters\n",
      "        ascii_only = True\n",
      "    else:\n",
      "        # Period, asterisk, and backtick\n",
      "        ascii_only = False\n",
      "    if not show_internal:\n",
      "        # Hide lighter nodes\n",
      "        ascii_only = False\n",
      "    else:\n",
      "        # Hide lighter nodes\n",
      "        ascii_only = True\n",
      "    result = \"\"\n",
      "    result += \"  \"\n",
      "    \n",
      "\n",
      "try:\n",
      "        # Close connection\n",
      "        self.connection.close()\n",
      "    except (ConnectionError, IOException) as e:\n",
      "        # Handle connection failure\n",
      "        print(\"Couldn't close connection: $e\")\n",
      "\n",
      "\n",
      "self.timeout = timeout\n",
      "    return self\n",
      "\n",
      "return self.connection.recv(buffer_size)\n",
      "\n",
      "\n",
      "while True:\n",
      "        buffer = await(self.buffer.read(buffer_size))\n",
      "        if not buffer: break\n",
      "        yield(buffer)\n",
      "\n",
      "\n",
      "with(msg)\n",
      "    return\n",
      "\n",
      "if value!= 0:\n",
      "        timeout *= 60\n",
      "        return_value = collections.OrderedDict()\n",
      "        try:\n",
      "            while True:\n",
      "                if value!= 0:\n",
      "                    value_str = value.toString()\n",
      "                else:\n",
      "                    value_str = \"\"\n",
      "                if value_str == cmd + value_str:\n",
      "                    break\n",
      "                time.sleep(1)\n",
      "                if timeout > 0:\n",
      "                    timeout -= 1\n",
      "                \n",
      "\n",
      "if not self.isAlive:\n",
      "        return\n",
      "    if self.isAliveString == \"true\":\n",
      "        try {\n",
      "            url = \"http://localhost:$port\"\n",
      "            print(\"Connecting to $url\")\n",
      "            print(url)\n",
      "            print(url.split(\"\"))\n",
      "            print(url.split(\".\")[0])\n",
      "            print(url.split(\".\")[1])\n",
      "            print(url.split(\".\")[2\n",
      "\n",
      "if isinstance(commands, str):\n",
      "        # String {\n",
      "        #     '/cmd:enableall /value:true'\n",
      "        # }\n",
      "        # ->\n",
      "        # String {\n",
      "        #     '\\\\camset\\n'\n",
      "        #     '\\\\camset\\n'\n",
      "        #     '\\\\camset\\n'\n",
      "        #     '\\\\camset\\n'\n",
      "        #     '\\\\camset\\n\n",
      "\n",
      "if self._closed: return\n",
      "    try:\n",
      "        # Close connection\n",
      "        self._connection.close()\n",
      "        # Reopen connection\n",
      "        self._connection = pkg.connection(self)\n",
      "        # Reopen class\n",
      "        class_ = self.__class__\n",
      "        class_._connection_ = self\n",
      "        class_._connection_close()\n",
      "        # Reopen list of classes\n",
      "        pkg.connection.listOf(class_)\n",
      "    except (\n",
      "\n",
      "@Logger(function.__name__) Logger.func(function.__name__)\n",
      "    @Logger.logger(function.__name__) Logger.logger(function.__name__)\n",
      "    @Logger.logger(function.__name__) Logger.afterlog(function.__name__)\n",
      "    @Logger.logger(function.__name__) Logger.beforelog(function.__name\n",
      "\n",
      "val job = (self.default_bar_value() / 100)\n",
      "    return (val job)\n",
      "}\n",
      "\n",
      "try:\n",
      "        # Close connection\n",
      "        self.connection.close()\n",
      "    except (ConnectionError, IOException) as e:\n",
      "        # Handle connection failure\n",
      "        print(\"Couldn't close connection: $e\")\n",
      "\n",
      "\n",
      "return q + \"`\"\n",
      "}\n",
      "\n",
      "return self.manifest.find(**params)\n",
      "\n",
      "\n",
      "if result is None:\n",
      "        return\n",
      "    if result.get(\"href\") == self._current_collection_href():\n",
      "        return result\n",
      "    else:\n",
      "        result[\"href\"] = self._current_collection_href()\n",
      "        return result\n",
      "\n",
      "\n",
      "return self.search(**params)\n",
      "\n",
      "\n",
      "writeText(section, value)\n",
      "    if not create: None {\n",
      "        remove(section)\n",
      "    }\n",
      "    if not os.path.exists(section): os.mkdir(section)\n",
      "    writeText(section, value)\n",
      "    if not os.path.exists(section): None\n",
      "\n",
      "\n",
      "val = S3CONF.get(section)\n",
      "    if not val:\n",
      "        print(\"No value for option %s in S3CONF: %s\" % (section, value))\n",
      "        sys.exit()\n",
      "    val = val.replace(\"\\n\", \" \")\n",
      "    val = val.replace(\"\\r\", \" \")\n",
      "    val = val.replace(\"\\n\", \" \")\n",
      "    val = val.replace(\"\\r\n",
      "\n",
      "ctx.invoke(lambda: editFile(create))\n",
      "    ctx.invoke(lambda: uploadFile(create))\n",
      "    ctx.invoke(lambda: downloadFile(create))\n",
      "    ctx.invoke(lambda: deleteFile(create))\n",
      "    ctx.invoke(lambda: changeFileName(create))\n",
      "    ctx.invoke(lambda: changeFileNameNoPermission(create))\n",
      "    ctx.invoke(lambda: changeFileNameNoPermissionWithErrorMessage(create\n",
      "\n",
      "configureLocalFile(section, remote_file)\n",
      "    configureRemoteFile(section, remote_file)\n",
      "    \n",
      "    configureLocalFile(section, \"CONFIG\")\n",
      "    configureRemoteFile(section, \"CONFIG\")\n",
      "    configureLocalFile(section, \"s3\")\n",
      "    configureRemoteFile(section, \"s3\")\n",
      "    configureLocalFile(section, \"s3.credentials\")\n",
      "    configureRemote\n",
      "\n",
      "return \"\"\"\n",
      "    return {\n",
      "        @Suppress(\"UNCHECKED_CAST\") val originalPrefix = `#jfrontend-twbs`;\n",
      "        @Suppress(\"UNCHECKED_CAST\") val activePrefix = `#jfrontend-active`;\n",
      "        @Suppress(\"UNCHECKED_CAST\") val iconI = `#jfrontend-icon`;\n",
      "        @Suppress(\"UNCHECKED_CAST\") val iconII =\n",
      "\n",
      "if (version == null) || (version == \"1.0\") || (version == \"1.1\") || (version == \"1.2\") || (version == \"1.3\") -> return minified()\n",
      "    return FILE_JAVA_frontend(version)\n",
      "\n",
      "\n",
      "if files is None:\n",
      "        files = []\n",
      "    if version is None: version = currentVerbose()\n",
      "    if version!= currentVerbose(): throw(\"djfrontend_twbs_js requires current verbose to be '%s', but was '%s'\" % (version, currentVerbose()))\n",
      "    files.sort()\n",
      "    if files.last() == \"__main__\" && version!= \"DEBUG\": throw(\"djfrontend\n",
      "\n",
      "if 'q' in kwargs:\n",
      "        q = kwargs['q']\n",
      "        if typeName == \"String\" && typeName == \"String\" && typeName == \"String\" && typeName == \"String\" && typeName == \"String\" == \"String\":\n",
      "            q = \"\\\"$q\\\"\"\n",
      "        else:\n",
      "            q = \"\\\"$q\\\"\"\n",
      "    if'start' in kwargs:\n",
      "        start = k\n",
      "\n",
      "url = \"https://www.zomato.com/rest/v1/restaurants/%s\" % restaurant_id\n",
      "    try:\n",
      "        response = self.session.get(url)\n",
      "        if response.statusCode!= 200:\n",
      "            print(\"Error during getting restaurant details:\")\n",
      "            print(response.text)\n",
      "        else:\n",
      "            return json.loads(response.text)\n",
      "    except (TimeoutException, TimeoutException)\n",
      "\n",
      "if (city_id == null) {\n",
      "        error(\"City ID is required\")\n",
      "    }\n",
      "    if (location!= null) {\n",
      "        error(\"City ID is required\")\n",
      "    }\n",
      "    if (coordinates!= null) {\n",
      "        error(\"City ID is required\")\n",
      "    }\n",
      "    if (area!= null) {\n",
      "        error(\"City ID is required\")\n",
      "    }\n",
      "    if (build!= null) {\n",
      "        error(\"City ID\n",
      "\n",
      "for (i, row) in enumerate(self.__parse().split(\"\\n\")):\n",
      "        for (j, cell) in enumerate(row.split(\"\\n\")):\n",
      "            try:\n",
      "                yield(Cell(i, j))\n",
      "            except Exception:\n",
      "                continue\n",
      "\n",
      "\n",
      "return self.__event_cells.columns\n",
      "\n",
      "\n",
      "return True\n",
      "\n",
      "\n",
      "return self.fetch(category, offset)\n",
      "\n",
      "metadata = item.metadata(filter=filter_classified)\n",
      "    for key in metadata:\n",
      "        try:\n",
      "            delattr(metadata[key], 'class')\n",
      "        except AttributeError: pass\n",
      "        try:\n",
      "            delattr(metadata[key], 'type')\n",
      "        except AttributeError: pass\n",
      "    return metadata\n",
      "\n",
      "\n",
      "return cls.is_fetch_process() && cls.is_item_fetch_process()\n",
      "}\n",
      "\n",
      "return self.fetch(category, **kwargs)\n",
      "\n",
      "\n",
      "parser = cls.make_parser(description = \"Generates reified type parameters for type parameters in the current module\")\n",
      "    parser.add(EOL)\n",
      "    return parser\n",
      "\n",
      "\n",
      "return sorted(curlentify2(from_page))\n",
      "\n",
      "return True\n",
      "\n",
      "\n",
      "return urlopen(url, payload).readreplace(\"\\n\", \"\")\n",
      "\n",
      "url = self.base_url + path\n",
      "    params = {\n",
      "        \"page\": page,\n",
      "        \"limit\": 1000\n",
      "    }\n",
      "    try:\n",
      "        response = self.session.get(url, params=params)\n",
      "    except (requests.exceptions.TimeoutException) as e:\n",
      "        print(\"Timeout: $path\")\n",
      "        print(e)\n",
      "        return\n",
      "    if not response.ok:\n",
      "        print(\"Error: $path\n",
      "\n",
      "if from_archive:\n",
      "        from.archives import ArchiveClient\n",
      "        return ArchiveClient()\n",
      "    else:\n",
      "        return Client()\n",
      "\n",
      "if item is Metadata.CATEGORY_summary.name:\n",
      "        return'summary'\n",
      "    elif item is Metadata.CATEGORY_crate.name:\n",
      "        return 'crate'\n",
      "    else:\n",
      "        val category = item.val\n",
      "        if category is Metadata.CATEGORY_extension.name:\n",
      "            return 'extension'\n",
      "        elif category is Metadata.CATEGORY_builtins\n",
      "\n",
      "parser = OptionParser(cls.__name__ + \" \" + cls.description)\n",
      "    parser.add(cls, \"crate\")\n",
      "    return parser\n",
      "\n",
      "\n",
      "\n",
      "url = \"%s/%s-summary\" % (self.base_url, category)\n",
      "    params = {\n",
      "        \"from\": from_date.isoformat(),\n",
      "        \"limit\": 10\n",
      "    }\n",
      "    try:\n",
      "        response = self.session.get(url, params = params)\n",
      "        if response.status_code!= 200:\n",
      "            print(\"Failed to fetch data: $url\")\n",
      "            return\n",
      "        else:\n",
      "            print\n",
      "\n",
      "return self.__summary\n",
      "}\n",
      "\n",
      "return self.fetch(question_id, \"question/answer\")\n",
      "\n",
      "return self.request.get_questions(offset)\n",
      "\n",
      "\n",
      "if from_archive:\n",
      "        from.archives import ArchiveClient\n",
      "        return ArchiveClient()\n",
      "    else:\n",
      "        return Client()\n",
      "\n",
      "return {\n",
      "        \"answer\": item.answer\n",
      "        \"type\": item.type\n",
      "        \"answerType\": item.answerType\n",
      "        \"question\": item.question\n",
      "        \"typeQuestion\": item.typeQuestion\n",
      "        \"answer\": item.answer\n",
      "        \"typeAnswer\": item.typeAnswer\n",
      "        \"answerType\": item.answerType\n",
      "        \"questionId\": item.questionId\n",
      "        \"typeQuestionId\": item.typeQuestionId\n",
      "        \"answer\n",
      "\n",
      "return self.session.post(self.base_url, data=data)\n",
      "\n",
      "try:\n",
      "        return urlparse(uri).getRequest()\n",
      "    except Exception as e:\n",
      "        print(\"Couldn't parse: $uri\")\n",
      "        print(e)\n",
      "        return None\n",
      "\n",
      "\n",
      "return _Response(statuscode = 400, content = \"\")\n",
      "\n",
      "\n",
      "try:\n",
      "        token = base64.urlsafe_base64_encode(hashlib.sha256((\"%s:%s:%s\" % (grant_type,client_id,client_secret)).toKString()).replace(\"\\n\",\"\"))\n",
      "        token = base64.urlsafe_base64_encode(hashlib.sha256((\"%s:%s:%s\" % (code,redirect_uri\n",
      "\n",
      "@property<EOL>private var _token_length: Int\n",
      "    \"\"\"\n",
      "    Property var to keep track of the length of the generated tokens.\n",
      "    \n",
      "            :rtype: Int\n",
      "    \"\"\"\n",
      "    @property<DEDENT>private var _indent: Int\n",
      "    \"\"\"\n",
      "    Property var to keep track of indentation.\n",
      "    \n",
      "            :rtype: Int\n",
      "    \"\"\"\n",
      "    @property<INDENT>private var _is_ind\n",
      "\n",
      "if exc is KtFileSystemException:\n",
      "        if self.isWindows():\n",
      "            throwable = KtFileSystemException\"\n",
      "        else:\n",
      "            throwable = KtFileSystemException\"\n",
      "        throwable.handle(exc)\n",
      "    else:\n",
      "        raise exc\n",
      "\n",
      "\n",
      "return self.__class__.token_provider.token(code, **params)\n",
      "\n",
      "\n",
      "return urlparse(url).query\n",
      "\n",
      "\n",
      "project = \"analysis/analysis-api\"\n",
      "    configure(project)\n",
      "    configureJpsKt()\n",
      "    projectPath = \"build/build-common\"\n",
      "    projectBuilder = projectPath.split(\"/\")\n",
      "    configureProjectBuilder(projectBuilder)\n",
      "    project = projectBuilder[0]\n",
      "    if (len(projectBuilder) > 1) and (projectBuilder[1] == \"test\"):\n",
      "        project = projectBuilder[0\n",
      "\n",
      "parser = ArgumentParser()\n",
      "    parser.addArgument(\"--outputFile\", \"-o\", default = \"compilerOutput.txt\", help = \"File to write compiler output to\")\n",
      "    parser.addArgument(\"--disableBase\", \"-b\", action = \"store\", default = false, help = \"Disable base derivation from default module\")\n",
      "    parser.addArgument(\"--disableJavac\", \"-b\", action = \"store\", default = false,\n",
      "\n",
      "with(open(fname, 'r') as f:\n",
      "        return json.load(f)\n",
      "\n",
      "\n",
      "return self.__removeEvent(name)\n",
      "}\n",
      "\n",
      "project.configure(services={})\n",
      "    project.configure {\n",
      "        services {\n",
      "            uris: uri -> {\n",
      "                if (uris.get()!= \"file\") throw AssertionError()\n",
      "            }\n",
      "        }\n",
      "        file(name) {\n",
      "            val file = File(this)\n",
      "            file.delete()\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "project.configure(services={})\n",
      "    project.configure {\n",
      "        services {\n",
      "            uris: uri -> {\n",
      "                if (uris.get()!= \"file\") throw AssertionError()\n",
      "            }\n",
      "        }\n",
      "        file(name) {\n",
      "            val file = File(this)\n",
      "            file.delete()\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "try:\n",
      "        fileEntry = FileEntryImpl(fileEntryDescriptor = content.fileEntryDescriptor)\n",
      "        fileEntry.fileEntryDescriptor = content\n",
      "        fileEntry.name = content.fileName\n",
      "        fileEntry.fileType = content.fileType\n",
      "        fileEntry.fileSize = content.fileSize\n",
      "        fileEntry.fileTimeStamp = content.fileTimeStamp\n",
      "        fileEntry.isDirectory = content.fileIs\n",
      "\n",
      "print(\"Request: $method$$url$headers$body\")\n",
      "    return\n",
      "\n",
      "    @property\n",
      "    def response(self): None {\n",
      "        print(\"Response: $this$\")\n",
      "        return\n",
      "    }\n",
      "}\n",
      "\n",
      "return self.args\n",
      "}\n",
      "\n",
      "def _check(value: String): None {\n",
      "        if (value.isEmpty()) {\n",
      "            receiver.empty = true\n",
      "        }\n",
      "    }\n",
      "    return _check\n",
      "\n",
      "\n",
      "return location.startswith(\"b\") and receiver.isValid &&!location.endswith(\"/\")\n",
      "\n",
      "\n",
      "try {\n",
      "        return(urlparse(self.url).scheme in self.scopes)\n",
      "    } catch (e: Exception) {\n",
      "        error(\"Error while checking: ${self.url}\")\n",
      "    }\n",
      "}\n",
      "\n",
      "dummy()\n",
      "    return True\n",
      "\n",
      "\n",
      "dummy()\n",
      "    return True\n",
      "\n",
      "\n",
      "freq = opt.get('freq')\n",
      "    config = opt.get('config')\n",
      "    messages = opt.get('messages')\n",
      "    return Service(freq, config, messages)\n",
      "\n",
      "\n",
      "timer = lambda: None\n",
      "    withWorker {\n",
      "        val date = Date.now()\n",
      "        val result = restarter(timer())\n",
      "        if (result!= listOf(\"A\", \"B\")) {\n",
      "            throw AssertionError(\"$restarter: $result\")\n",
      "        }\n",
      "        val date2 = Date.now()\n",
      "        assertEquals(\"A\", result)\n",
      "        assertEquals(\"B\", restocker(date2))\n",
      "    }\n",
      "\n",
      "process = Process(args, timeout, grace)\n",
      "    process.start()\n",
      "    returnDeferred(process.exitValue, process)\n",
      "\n",
      "\n",
      "pass\n",
      "\n",
      "\n",
      "pass\n",
      "\n",
      "\n",
      "if myEnv!= null: EnvUtils.set(myEnv)\n",
      "    return CaseEnvironmentManager.replaceEnvironment(case)\n",
      "\n",
      "\n",
      "if heart is not null:\n",
      "        heart.maybeAddHeart(master)\n",
      "\n",
      "\n",
      "try:\n",
      "        value = self._json.get('value')\n",
      "        log = self._log.get(value)\n",
      "        if value =='restart' and log!= \"OK\":\n",
      "            log = \"fail: $value\"\n",
      "        elif value == \"restart-all\" and log!= \"OK\":\n",
      "            log = \"fail: $value\"\n",
      "        else:\n",
      "            log = \"\"\n",
      "        message = \"\"\"\n",
      "                $value::${log\n",
      "\n",
      "try:\n",
      "        process = self.processQueue.remove(name)\n",
      "        if not process: throw RuntimeException(\"Could not remove process '$name'\")\n",
      "        return process\n",
      "    except RuntimeException as e:\n",
      "        if e.message!= \"Process '$name' was not found in the process queue: $name\" and e.message!= \"Process '$name' was not marked as 'TERMINATED'\" and e.message!= \"Process '$name\n",
      "\n",
      "parser = OptionParser(description=\"\"\"\n",
      "                Simple CLI entry point for the $LCA_PROGRAM_NAME\n",
      "                application. Can be used as a standalone script\n",
      "                or as a daemonized process.\n",
      "                \n",
      "                Usage: <PROGRAM_NAME> <options>\n",
      "    \n",
      "                Options:\n",
      "                    <STR_LIT>\n",
      "                        [OPTION]\n",
      "                            --messages=messages directory\n",
      "                            \n",
      "                            --config=config directory\n",
      "\n",
      "\n",
      "val = places.execute(name, \"Restart\")\n",
      "    if val!= 0:\n",
      "        throw RuntimeException(\"Could not restart process '%s'\" % name)\n",
      "\n",
      "\n",
      "freq = opt.get('freq')\n",
      "    config = opt.get('config')\n",
      "    messages = opt.get('messages')\n",
      "    return Service(freq, config, messages)\n",
      "\n",
      "\n",
      "while True:\n",
      "        val = timer()\n",
      "        val2 = restarter(val)\n",
      "        if val2!= val:\n",
      "            break\n",
      "    if (timer() - val) > 0.1:\n",
      "        sys.exit(\"Timed out: \" + str(timer() - val))\n",
      "    sys.exit(\"OK\")\n",
      "\n",
      "\n",
      "(restarter, path) = parseBaseConfig(opt)\n",
      "    return parseFirConfig(path)\n",
      "\n",
      "\n",
      "return keccak256(data)\n",
      "\n",
      "\n",
      "return {\n",
      "       'request': 'create',\n",
      "        'id': 'create',\n",
      "        'value': {\n",
      "            'pid': pid_value,\n",
      "            'users': users,\n",
      "            'confirmed': confirmed\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "{'accuracy score': 0.02, 'bleu score': 0.004466778020517986}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " {\n",
      "    val x = test()\n",
      "    if (x!= stepId) {\n",
      "        return \"Fail: $x!= $stepId\"\n",
      "    }\n",
      "    return \"OK\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "c + d\n",
      "\n",
      "\n",
      "\n",
      "ilt(destination, startIndex, endIndex)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(Color::class, arg)\n",
      "\n",
      "\n",
      "\n",
      "if (targetName == null) project.targets.getOrPut(name) else JsDefaultDistribution(project, targetName)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "setter(this, property, string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(first, second)\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "atan(x)\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "args.toList()\n",
      "\n",
      "ilt.toString()\n",
      "\n",
      "\n",
      "\n",
      "ilt(true)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ensureEquals(actual, expected)\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(value.toULong()).toULong()\n",
      "\n",
      "\n",
      "\n",
      "ilt(0)\n",
      "\n",
      "stableOpenClass.lastDefaultValueInFunction(a = n)\n",
      "\n",
      "\n",
      "\n",
      "isFinallyMarker(node, INLINE_MARKER_FINALLY_END)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "    var i = key.computeHash(aShift)\n",
      "    while (true) {\n",
      "        val k = array[i]\n",
      "        if (k == null) {\n",
      "            array[i] = key\n",
      "            array[i + 1] = value\n",
      "            return true\n",
      "        }\n",
      "        if (k == key) break\n",
      "        if (i == 0) {\n",
      "            i = array.size\n",
      "        }\n",
      "        i -= 2\n",
      "    \n",
      "\n",
      "ilt(block)\n",
      "\n",
      "c\n",
      "\n",
      "\n",
      "\n",
      "ilt(listOf(modules), context, moduleKind)\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "if (descriptor is CallableDescriptor && descriptor.overriddenDescriptors.isEmpty()) {\n",
      "    overriddenDescriptors.add(descriptor)\n",
      "} else {\n",
      "    val originalSignature = descriptor.originalSignature?: return null\n",
      "    if (originalSignature.isBadJavaSignature()) {\n",
      "        descriptor.overriddenDescriptors.add(overriddenDescriptors)\n",
      "    }\n",
      "}\n",
      "\n",
      "data.copy()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " {\n",
      "    return storage.lastIndexOf(element.toInt())\n",
      "}\n",
      "\n",
      "a.s = w(a)\n",
      "\n",
      "f()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      " {\n",
      "    if (2.toShort() in emptyCharSequence.indices!= range1.contains(2.toShort())) throw AssertionError()\n",
      "    if (2.toShort()!in emptyCharSequence.indices!=!range1.contains(2.toShort())) throw AssertionError()\n",
      "    if (!(2.toShort() in emptyCharSequence.indices)!=!range1.cont\n",
      "\n",
      "ilt(node, visitor)\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "remainder(v1.toLong(), v2.toLong()).toUInt()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iltocode<String>(compilerId, \"main\")\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "ilt(xNotNull())\n",
      "\n",
      "ilt(void)\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "  consume(block)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(from, to)\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(x:?, y:?).let { ilt(x, y) }\n",
      "\n",
      "acos(x)\n",
      "\n",
      "\n",
      "\n",
      "{'accuracy score': 0.06, 'bleu score': 8.629398069945111e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3028571406726984, 0.12307692215088759, 0.2980952359107937)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(utils.evaluate(model, tokenizer, utils.CodeCompletionDataset(test_codexglue_dataset.prompts[:100], test_codexglue_dataset.answers[:100], train=False)))\n",
    "print(utils.evaluate(model, tokenizer, utils.CodeCompletionDataset(test_kotlin_dataset.prompts[:100], test_kotlin_dataset.answers[:100], train=False)))\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "completions = []\n",
    "completion_tokens = []\n",
    "input_tokens = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, tokenizer, eval_dataset, *, max_new_tokens: int = 100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt, answer in tqdm(eval_dataset):\n",
    "            prompt_tensor = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            input_tokens.append(prompt_tensor)\n",
    "            output = model.generate(\n",
    "                prompt_tensor,\n",
    "                min_new_tokens=2,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                bos_token_id=tokenizer.bos_token_id,\n",
    "            )\n",
    "            completion_tokens.append(output[0])\n",
    "            completions.append(tokenizer.decode(output[0][len(prompt_tensor[0]):], skip_special_tokens=True))\n",
    "\n",
    "            answers.append(answer)\n",
    "    # for compl in completions:\n",
    "    #     print(compl)\n",
    "    #     print()\n",
    "\n",
    "    return {\n",
    "        \"accuracy score\": accuracy_score(answers, completions),\n",
    "        \"bleu score\": corpus_bleu([[ans.split()] for ans in answers], [comp.split() for comp in completions]),\n",
    "    }\n",
    "\n",
    "\n",
    "evaluate(model, tokenizer, utils.CodeCompletionDataset(test_kotlin_dataset.prompts[:10], test_kotlin_dataset.answers[:10], train=False))\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(completions, answers, avg=True)\n",
    "scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20240503_225725-6yn3anj8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/finetune/runs/6yn3anj8' target=\"_blank\">wild-wood-16</a></strong> to <a href='https://wandb.ai/antonii-belyshev/finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/finetune' target=\"_blank\">https://wandb.ai/antonii-belyshev/finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/finetune/runs/6yn3anj8' target=\"_blank\">https://wandb.ai/antonii-belyshev/finetune/runs/6yn3anj8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Loss: 0.6368: 100%|██████████| 20026/20026 [57:47<00:00,  5.78it/s]  \n",
      "Epoch [2/8], Loss: 1.2122: 100%|██████████| 20026/20026 [57:51<00:00,  5.77it/s]  \n",
      "Epoch [3/8], Loss: 0.4741:  30%|██▉       | 5983/20026 [17:08<40:18,  5.81it/s]  "
     ]
    }
   ],
   "source": [
    "model = utils.train_model(model, tokenizer, train_dataset, epochs=8, learning_rate=1e-7, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:47<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (user.isVerbose):\n",
      "        print(\"$DEBUG: $message\")\n",
      "\n",
      "\n",
      "if isSuccess(user):\n",
      "        message = \"[$HAS_SUCCESS] $user\"\n",
      "    elif isWarning(user):\n",
      "        message = \"[$HAS_WARNING] $user\"\n",
      "    elif isError(user):\n",
      "        message = \"[$HAS_ERROR] $user\"\n",
      "    else:\n",
      "        message = \"[$HAS_LOG] $user\"\n",
      "    user.l += message\n",
      "    if LOG!= \"console\n",
      "\n",
      "try:\n",
      "        user = self.user_lookup(request)\n",
      "        if user is not None:\n",
      "            response.processAll(user.receiveMessages())\n",
      "        else:\n",
      "            response.process(\"fail: user not found\")\n",
      "    except Exception:\n",
      "        response.process(\"fail: exception processing\")\n",
      "\n",
      "\n",
      "return {\n",
      "        table(\"user\", \"$name\", \"$auth\")\n",
      "        table(\"user\", \"$name\", \"$auth\", eager = eager)\n",
      "        table(\"user\", \"$name\", \"$auth\", \"caches\")\n",
      "    }[name]\n",
      "\n",
      "\n",
      "@wraps(this)\n",
      "    @require(methods) { \"require(methods.isNotEmpty() == true) { \" }\n",
      "    @require(it == typeOf<String>()) { \"Type of method is not string: $this\" }\n",
      "    require(it == typeOf<String?>()) { \"Type of method is not string: $this\" }\n",
      "    @require(it == typeOf<String?>()) { \"Type\n",
      "\n",
      "user = TwitterUser(user)\n",
      "    user.login()\n",
      "    return {\n",
      "        i: tweet for i, tweet in enumerate(user.search(user.followed_user.id, pages)!!)\n",
      "        if tweet is not null\n",
      "    }\n",
      "\n",
      "\n",
      "deformation_42 = []\n",
      "    for i in range(len(chn_names)):\n",
      "        deformation_42.append(data[i,:,:,2])\n",
      "    return deformation_42\n",
      "\n",
      "\n",
      "return self.get(LighterASTNode.Leaf)\n",
      "\n",
      "return self.get(label)\n",
      "\n",
      "\n",
      "return self.get(LighterASTNode.LeafNode)()\n",
      "\n",
      "\n",
      "return cls(name, length, descendants, **kw)\n",
      "\n",
      "\n",
      "return self.__newick\n",
      "}\n",
      "\n",
      "result = []\n",
      "    try:\n",
      "        result.append(Node.from(s, **kw))\n",
      "    except(NodeException) as e:\n",
      "        raise NodeException(\"Node is not a New YorkTree: $s\") from e\n",
      "    if strip_comments:\n",
      "        for i in range(len(result)):\n",
      "            result[i] = removeComments(result[i], stripComments = True)\n",
      "    return result\n",
      "\n",
      "\n",
      "if predicate is None:\n",
      "        predicate = lambda node: True\n",
      "    for node in self.walk(predicate, **kw)\n",
      "    return visitor(node)\n",
      "\n",
      "\n",
      "if strict:\n",
      "        # Only ASCII characters\n",
      "        ascii_only = True\n",
      "    else:\n",
      "        # Period, asterisk, and backtick\n",
      "        ascii_only = False\n",
      "    if not show_internal:\n",
      "        # Hide lighter nodes\n",
      "        ascii_only = False\n",
      "    else:\n",
      "        # Hide lighter nodes\n",
      "        ascii_only = True\n",
      "    result = \"\"\n",
      "    result += \"  \"\n",
      "    \n",
      "\n",
      "try:\n",
      "        # Close connection\n",
      "        self.connection.close()\n",
      "    except (ConnectionError, IOException) as e:\n",
      "        # Handle connection failure\n",
      "        print(\"Couldn't close connection: $e\")\n",
      "\n",
      "\n",
      "self.timeout = timeout\n",
      "    return self\n",
      "\n",
      "return self.connection.recv(buffer_size)\n",
      "\n",
      "\n",
      "while True:\n",
      "        buffer = await(self.buffer.read(buffer_size))\n",
      "        if not buffer: break\n",
      "        yield(buffer)\n",
      "\n",
      "\n",
      "with(msg)\n",
      "    return\n",
      "\n",
      "if value!= 0:\n",
      "        timeout *= 60\n",
      "        return_value = collections.OrderedDict()\n",
      "        try:\n",
      "            while True:\n",
      "                if value!= 0:\n",
      "                    value_str = value.toString()\n",
      "                else:\n",
      "                    value_str = \"\"\n",
      "                if value_str == cmd + value_str:\n",
      "                    break\n",
      "                time.sleep(1)\n",
      "                if timeout > 0:\n",
      "                    timeout -= 1\n",
      "                \n",
      "\n",
      "if not self.isAlive:\n",
      "        return\n",
      "    if self.isAliveString == \"true\":\n",
      "        try {\n",
      "            url = \"http://localhost:$port\"\n",
      "            print(\"Connecting to $url\")\n",
      "            print(url)\n",
      "            print(url.split(\"\"))\n",
      "            print(url.split(\".\")[0])\n",
      "            print(url.split(\".\")[1])\n",
      "            print(url.split(\".\")[2\n",
      "\n",
      "if isinstance(commands, str):\n",
      "        # String {\n",
      "        #     '/cmd:enableall /value:true'\n",
      "        # }\n",
      "        # ->\n",
      "        # String {\n",
      "        #     '\\\\camset\\n'\n",
      "        #     '\\\\camset\\n'\n",
      "        #     '\\\\camset\\n'\n",
      "        #     '\\\\camset\\n'\n",
      "        #     '\\\\camset\\n\n",
      "\n",
      "if self._closed: return\n",
      "    try:\n",
      "        # Close connection\n",
      "        self._connection.close()\n",
      "        # Reopen connection\n",
      "        self._connection = pkg.connection(self)\n",
      "        # Reopen class\n",
      "        class_ = self.__class__\n",
      "        class_._connection_ = self\n",
      "        class_._connection_close()\n",
      "        # Reopen list of classes\n",
      "        pkg.connection.listOf(class_)\n",
      "    except (\n",
      "\n",
      "@Logger(function.__name__) Logger.func(function.__name__)\n",
      "    @Logger.logger(function.__name__) Logger.logger(function.__name__)\n",
      "    @Logger.logger(function.__name__) Logger.afterlog(function.__name__)\n",
      "    @Logger.logger(function.__name__) Logger.beforelog(function.__name\n",
      "\n",
      "val job = (self.default_bar_value() / 100)\n",
      "    return (val job)\n",
      "}\n",
      "\n",
      "try:\n",
      "        # Close connection\n",
      "        self.connection.close()\n",
      "    except (ConnectionError, IOException) as e:\n",
      "        # Handle connection failure\n",
      "        print(\"Couldn't close connection: $e\")\n",
      "\n",
      "\n",
      "return q + \"`\"\n",
      "}\n",
      "\n",
      "return self.manifest.find(**params)\n",
      "\n",
      "\n",
      "if result is None:\n",
      "        return\n",
      "    if result.get(\"href\") == self._current_collection_href():\n",
      "        return result\n",
      "    else:\n",
      "        result[\"href\"] = self._current_collection_href()\n",
      "        return result\n",
      "\n",
      "\n",
      "return self.search(**params)\n",
      "\n",
      "\n",
      "writeText(section, value)\n",
      "    if not create: None {\n",
      "        remove(section)\n",
      "    }\n",
      "    if not os.path.exists(section): os.mkdir(section)\n",
      "    writeText(section, value)\n",
      "    if not os.path.exists(section): None\n",
      "\n",
      "\n",
      "val = S3CONF.get(section)\n",
      "    if not val:\n",
      "        print(\"No value for option %s in S3CONF: %s\" % (section, value))\n",
      "        sys.exit()\n",
      "    val = val.replace(\"\\n\", \" \")\n",
      "    val = val.replace(\"\\r\", \" \")\n",
      "    val = val.replace(\"\\n\", \" \")\n",
      "    val = val.replace(\"\\r\n",
      "\n",
      "ctx.invoke(lambda: editFile(create))\n",
      "    ctx.invoke(lambda: uploadFile(create))\n",
      "    ctx.invoke(lambda: downloadFile(create))\n",
      "    ctx.invoke(lambda: deleteFile(create))\n",
      "    ctx.invoke(lambda: changeFileName(create))\n",
      "    ctx.invoke(lambda: changeFileNameNoPermission(create))\n",
      "    ctx.invoke(lambda: changeFileNameNoPermissionWithErrorMessage(create\n",
      "\n",
      "configureLocalFile(section, remote_file)\n",
      "    configureRemoteFile(section, remote_file)\n",
      "    \n",
      "    configureLocalFile(section, \"CONFIG\")\n",
      "    configureRemoteFile(section, \"CONFIG\")\n",
      "    configureLocalFile(section, \"s3\")\n",
      "    configureRemoteFile(section, \"s3\")\n",
      "    configureLocalFile(section, \"s3.credentials\")\n",
      "    configureRemote\n",
      "\n",
      "return \"\"\"\n",
      "    return {\n",
      "        @Suppress(\"UNCHECKED_CAST\") val originalPrefix = `#jfrontend-twbs`;\n",
      "        @Suppress(\"UNCHECKED_CAST\") val activePrefix = `#jfrontend-active`;\n",
      "        @Suppress(\"UNCHECKED_CAST\") val iconI = `#jfrontend-icon`;\n",
      "        @Suppress(\"UNCHECKED_CAST\") val iconII =\n",
      "\n",
      "if (version == null) || (version == \"1.0\") || (version == \"1.1\") || (version == \"1.2\") || (version == \"1.3\") -> return minified()\n",
      "    return FILE_JAVA_frontend(version)\n",
      "\n",
      "\n",
      "if files is None:\n",
      "        files = []\n",
      "    if version is None: version = currentVerbose()\n",
      "    if version!= currentVerbose(): throw(\"djfrontend_twbs_js requires current verbose to be '%s', but was '%s'\" % (version, currentVerbose()))\n",
      "    files.sort()\n",
      "    if files.last() == \"__main__\" && version!= \"DEBUG\": throw(\"djfrontend\n",
      "\n",
      "if 'q' in kwargs:\n",
      "        q = kwargs['q']\n",
      "        if typeName == \"String\" && typeName == \"String\" && typeName == \"String\" && typeName == \"String\" && typeName == \"String\" == \"String\":\n",
      "            q = \"\\\"$q\\\"\"\n",
      "        else:\n",
      "            q = \"\\\"$q\\\"\"\n",
      "    if'start' in kwargs:\n",
      "        start = k\n",
      "\n",
      "url = \"https://www.zomato.com/rest/v1/restaurants/%s\" % restaurant_id\n",
      "    try:\n",
      "        response = self.session.get(url)\n",
      "        if response.statusCode!= 200:\n",
      "            print(\"Error during getting restaurant details:\")\n",
      "            print(response.text)\n",
      "        else:\n",
      "            return json.loads(response.text)\n",
      "    except (TimeoutException, TimeoutException)\n",
      "\n",
      "if (city_id == null) {\n",
      "        error(\"City ID is required\")\n",
      "    }\n",
      "    if (location!= null) {\n",
      "        error(\"City ID is required\")\n",
      "    }\n",
      "    if (coordinates!= null) {\n",
      "        error(\"City ID is required\")\n",
      "    }\n",
      "    if (area!= null) {\n",
      "        error(\"City ID is required\")\n",
      "    }\n",
      "    if (build!= null) {\n",
      "        error(\"City ID\n",
      "\n",
      "for (i, row) in enumerate(self.__parse().split(\"\\n\")):\n",
      "        for (j, cell) in enumerate(row.split(\"\\n\")):\n",
      "            try:\n",
      "                yield(Cell(i, j))\n",
      "            except Exception:\n",
      "                continue\n",
      "\n",
      "\n",
      "return self.__event_cells.columns\n",
      "\n",
      "\n",
      "return True\n",
      "\n",
      "\n",
      "return self.fetch(category, offset)\n",
      "\n",
      "metadata = item.metadata(filter=filter_classified)\n",
      "    for key in metadata:\n",
      "        try:\n",
      "            delattr(metadata[key], 'class')\n",
      "        except AttributeError: pass\n",
      "        try:\n",
      "            delattr(metadata[key], 'type')\n",
      "        except AttributeError: pass\n",
      "    return metadata\n",
      "\n",
      "\n",
      "return cls.is_fetch_process() && cls.is_item_fetch_process()\n",
      "}\n",
      "\n",
      "return self.fetch(category, **kwargs)\n",
      "\n",
      "\n",
      "parser = cls.make_parser(description = \"Generates reified type parameters for type parameters in the current module\")\n",
      "    parser.add(EOL)\n",
      "    return parser\n",
      "\n",
      "\n",
      "return sorted(curlentify2(from_page))\n",
      "\n",
      "return True\n",
      "\n",
      "\n",
      "return urlopen(url, payload).readreplace(\"\\n\", \"\")\n",
      "\n",
      "url = self.base_url + path\n",
      "    params = {\n",
      "        \"page\": page,\n",
      "        \"limit\": 1000\n",
      "    }\n",
      "    try:\n",
      "        response = self.session.get(url, params=params)\n",
      "    except (requests.exceptions.TimeoutException) as e:\n",
      "        print(\"Timeout: $path\")\n",
      "        print(e)\n",
      "        return\n",
      "    if not response.ok:\n",
      "        print(\"Error: $path\n",
      "\n",
      "if from_archive:\n",
      "        from.archives import ArchiveClient\n",
      "        return ArchiveClient()\n",
      "    else:\n",
      "        return Client()\n",
      "\n",
      "if item is Metadata.CATEGORY_summary.name:\n",
      "        return'summary'\n",
      "    elif item is Metadata.CATEGORY_crate.name:\n",
      "        return 'crate'\n",
      "    else:\n",
      "        val category = item.val\n",
      "        if category is Metadata.CATEGORY_extension.name:\n",
      "            return 'extension'\n",
      "        elif category is Metadata.CATEGORY_builtins\n",
      "\n",
      "parser = OptionParser(cls.__name__ + \" \" + cls.description)\n",
      "    parser.add(cls, \"crate\")\n",
      "    return parser\n",
      "\n",
      "\n",
      "\n",
      "url = \"%s/%s-summary\" % (self.base_url, category)\n",
      "    params = {\n",
      "        \"from\": from_date.isoformat(),\n",
      "        \"limit\": 10\n",
      "    }\n",
      "    try:\n",
      "        response = self.session.get(url, params = params)\n",
      "        if response.status_code!= 200:\n",
      "            print(\"Failed to fetch data: $url\")\n",
      "            return\n",
      "        else:\n",
      "            print\n",
      "\n",
      "return self.__summary\n",
      "}\n",
      "\n",
      "return self.fetch(question_id, \"question/answer\")\n",
      "\n",
      "return self.request.get_questions(offset)\n",
      "\n",
      "\n",
      "if from_archive:\n",
      "        from.archives import ArchiveClient\n",
      "        return ArchiveClient()\n",
      "    else:\n",
      "        return Client()\n",
      "\n",
      "return {\n",
      "        \"answer\": item.answer\n",
      "        \"type\": item.type\n",
      "        \"answerType\": item.answerType\n",
      "        \"question\": item.question\n",
      "        \"typeQuestion\": item.typeQuestion\n",
      "        \"answer\": item.answer\n",
      "        \"typeAnswer\": item.typeAnswer\n",
      "        \"answerType\": item.answerType\n",
      "        \"questionId\": item.questionId\n",
      "        \"typeQuestionId\": item.typeQuestionId\n",
      "        \"answer\n",
      "\n",
      "return self.session.post(self.base_url, data=data)\n",
      "\n",
      "try:\n",
      "        return urlparse(uri).getRequest()\n",
      "    except Exception as e:\n",
      "        print(\"Couldn't parse: $uri\")\n",
      "        print(e)\n",
      "        return None\n",
      "\n",
      "\n",
      "return _Response(statuscode = 400, content = \"\")\n",
      "\n",
      "\n",
      "try:\n",
      "        token = base64.urlsafe_base64_encode(hashlib.sha256((\"%s:%s:%s\" % (grant_type,client_id,client_secret)).toKString()).replace(\"\\n\",\"\"))\n",
      "        token = base64.urlsafe_base64_encode(hashlib.sha256((\"%s:%s:%s\" % (code,redirect_uri\n",
      "\n",
      "@property<EOL>private var _token_length: Int\n",
      "    \"\"\"\n",
      "    Property var to keep track of the length of the generated tokens.\n",
      "    \n",
      "            :rtype: Int\n",
      "    \"\"\"\n",
      "    @property<DEDENT>private var _indent: Int\n",
      "    \"\"\"\n",
      "    Property var to keep track of indentation.\n",
      "    \n",
      "            :rtype: Int\n",
      "    \"\"\"\n",
      "    @property<INDENT>private var _is_ind\n",
      "\n",
      "if exc is KtFileSystemException:\n",
      "        if self.isWindows():\n",
      "            throwable = KtFileSystemException\"\n",
      "        else:\n",
      "            throwable = KtFileSystemException\"\n",
      "        throwable.handle(exc)\n",
      "    else:\n",
      "        raise exc\n",
      "\n",
      "\n",
      "return self.__class__.token_provider.token(code, **params)\n",
      "\n",
      "\n",
      "return urlparse(url).query\n",
      "\n",
      "\n",
      "project = \"analysis/analysis-api\"\n",
      "    configure(project)\n",
      "    configureJpsKt()\n",
      "    projectPath = \"build/build-common\"\n",
      "    projectBuilder = projectPath.split(\"/\")\n",
      "    configureProjectBuilder(projectBuilder)\n",
      "    project = projectBuilder[0]\n",
      "    if (len(projectBuilder) > 1) and (projectBuilder[1] == \"test\"):\n",
      "        project = projectBuilder[0\n",
      "\n",
      "parser = ArgumentParser()\n",
      "    parser.addArgument(\"--outputFile\", \"-o\", default = \"compilerOutput.txt\", help = \"File to write compiler output to\")\n",
      "    parser.addArgument(\"--disableBase\", \"-b\", action = \"store\", default = false, help = \"Disable base derivation from default module\")\n",
      "    parser.addArgument(\"--disableJavac\", \"-b\", action = \"store\", default = false,\n",
      "\n",
      "with(open(fname, 'r') as f:\n",
      "        return json.load(f)\n",
      "\n",
      "\n",
      "return self.__removeEvent(name)\n",
      "}\n",
      "\n",
      "project.configure(services={})\n",
      "    project.configure {\n",
      "        services {\n",
      "            uris: uri -> {\n",
      "                if (uris.get()!= \"file\") throw AssertionError()\n",
      "            }\n",
      "        }\n",
      "        file(name) {\n",
      "            val file = File(this)\n",
      "            file.delete()\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "project.configure(services={})\n",
      "    project.configure {\n",
      "        services {\n",
      "            uris: uri -> {\n",
      "                if (uris.get()!= \"file\") throw AssertionError()\n",
      "            }\n",
      "        }\n",
      "        file(name) {\n",
      "            val file = File(this)\n",
      "            file.delete()\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "try:\n",
      "        fileEntry = FileEntryImpl(fileEntryDescriptor = content.fileEntryDescriptor)\n",
      "        fileEntry.fileEntryDescriptor = content\n",
      "        fileEntry.name = content.fileName\n",
      "        fileEntry.fileType = content.fileType\n",
      "        fileEntry.fileSize = content.fileSize\n",
      "        fileEntry.fileTimeStamp = content.fileTimeStamp\n",
      "        fileEntry.isDirectory = content.fileIs\n",
      "\n",
      "print(\"Request: $method$$url$headers$body\")\n",
      "    return\n",
      "\n",
      "    @property\n",
      "    def response(self): None {\n",
      "        print(\"Response: $this$\")\n",
      "        return\n",
      "    }\n",
      "}\n",
      "\n",
      "return self.args\n",
      "}\n",
      "\n",
      "def _check(value: String): None {\n",
      "        if (value.isEmpty()) {\n",
      "            receiver.empty = true\n",
      "        }\n",
      "    }\n",
      "    return _check\n",
      "\n",
      "\n",
      "return location.startswith(\"b\") and receiver.isValid &&!location.endswith(\"/\")\n",
      "\n",
      "\n",
      "try {\n",
      "        return(urlparse(self.url).scheme in self.scopes)\n",
      "    } catch (e: Exception) {\n",
      "        error(\"Error while checking: ${self.url}\")\n",
      "    }\n",
      "}\n",
      "\n",
      "dummy()\n",
      "    return True\n",
      "\n",
      "\n",
      "dummy()\n",
      "    return True\n",
      "\n",
      "\n",
      "freq = opt.get('freq')\n",
      "    config = opt.get('config')\n",
      "    messages = opt.get('messages')\n",
      "    return Service(freq, config, messages)\n",
      "\n",
      "\n",
      "timer = lambda: None\n",
      "    withWorker {\n",
      "        val date = Date.now()\n",
      "        val result = restarter(timer())\n",
      "        if (result!= listOf(\"A\", \"B\")) {\n",
      "            throw AssertionError(\"$restarter: $result\")\n",
      "        }\n",
      "        val date2 = Date.now()\n",
      "        assertEquals(\"A\", result)\n",
      "        assertEquals(\"B\", restocker(date2))\n",
      "    }\n",
      "\n",
      "process = Process(args, timeout, grace)\n",
      "    process.start()\n",
      "    returnDeferred(process.exitValue, process)\n",
      "\n",
      "\n",
      "pass\n",
      "\n",
      "\n",
      "pass\n",
      "\n",
      "\n",
      "if myEnv!= null: EnvUtils.set(myEnv)\n",
      "    return CaseEnvironmentManager.replaceEnvironment(case)\n",
      "\n",
      "\n",
      "if heart is not null:\n",
      "        heart.maybeAddHeart(master)\n",
      "\n",
      "\n",
      "try:\n",
      "        value = self._json.get('value')\n",
      "        log = self._log.get(value)\n",
      "        if value =='restart' and log!= \"OK\":\n",
      "            log = \"fail: $value\"\n",
      "        elif value == \"restart-all\" and log!= \"OK\":\n",
      "            log = \"fail: $value\"\n",
      "        else:\n",
      "            log = \"\"\n",
      "        message = \"\"\"\n",
      "                $value::${log\n",
      "\n",
      "try:\n",
      "        process = self.processQueue.remove(name)\n",
      "        if not process: throw RuntimeException(\"Could not remove process '$name'\")\n",
      "        return process\n",
      "    except RuntimeException as e:\n",
      "        if e.message!= \"Process '$name' was not found in the process queue: $name\" and e.message!= \"Process '$name' was not marked as 'TERMINATED'\" and e.message!= \"Process '$name\n",
      "\n",
      "parser = OptionParser(description=\"\"\"\n",
      "                Simple CLI entry point for the $LCA_PROGRAM_NAME\n",
      "                application. Can be used as a standalone script\n",
      "                or as a daemonized process.\n",
      "                \n",
      "                Usage: <PROGRAM_NAME> <options>\n",
      "    \n",
      "                Options:\n",
      "                    <STR_LIT>\n",
      "                        [OPTION]\n",
      "                            --messages=messages directory\n",
      "                            \n",
      "                            --config=config directory\n",
      "\n",
      "\n",
      "val = places.execute(name, \"Restart\")\n",
      "    if val!= 0:\n",
      "        throw RuntimeException(\"Could not restart process '%s'\" % name)\n",
      "\n",
      "\n",
      "freq = opt.get('freq')\n",
      "    config = opt.get('config')\n",
      "    messages = opt.get('messages')\n",
      "    return Service(freq, config, messages)\n",
      "\n",
      "\n",
      "while True:\n",
      "        val = timer()\n",
      "        val2 = restarter(val)\n",
      "        if val2!= val:\n",
      "            break\n",
      "    if (timer() - val) > 0.1:\n",
      "        sys.exit(\"Timed out: \" + str(timer() - val))\n",
      "    sys.exit(\"OK\")\n",
      "\n",
      "\n",
      "(restarter, path) = parseBaseConfig(opt)\n",
      "    return parseFirConfig(path)\n",
      "\n",
      "\n",
      "return keccak256(data)\n",
      "\n",
      "\n",
      "return {\n",
      "       'request': 'create',\n",
      "        'id': 'create',\n",
      "        'value': {\n",
      "            'pid': pid_value,\n",
      "            'users': users,\n",
      "            'confirmed': confirmed\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy score': 0.02, 'bleu score': 0.004466778020517986}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.evaluate(model, tokenizer, utils.CodeCompletionDataset(test_codexglue_dataset.prompts[:100], test_codexglue_dataset.answers[:100], train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " {\n",
      "    val x = test()\n",
      "    if (x!= stepId) {\n",
      "        return \"Fail: $x!= $stepId\"\n",
      "    }\n",
      "    return \"OK\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "c + d\n",
      "\n",
      "\n",
      "\n",
      "ilt(destination, startIndex, endIndex)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(Color::class, arg)\n",
      "\n",
      "\n",
      "\n",
      "if (targetName == null) project.targets.getOrPut(name) else JsDefaultDistribution(project, targetName)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "setter(this, property, string)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(first, second)\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "atan(x)\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "args.toList()\n",
      "\n",
      "ilt.toString()\n",
      "\n",
      "\n",
      "\n",
      "ilt(true)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ensureEquals(actual, expected)\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(value.toULong()).toULong()\n",
      "\n",
      "\n",
      "\n",
      "ilt(0)\n",
      "\n",
      "stableOpenClass.lastDefaultValueInFunction(a = n)\n",
      "\n",
      "\n",
      "\n",
      "isFinallyMarker(node, INLINE_MARKER_FINALLY_END)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "    var i = key.computeHash(aShift)\n",
      "    while (true) {\n",
      "        val k = array[i]\n",
      "        if (k == null) {\n",
      "            array[i] = key\n",
      "            array[i + 1] = value\n",
      "            return true\n",
      "        }\n",
      "        if (k == key) break\n",
      "        if (i == 0) {\n",
      "            i = array.size\n",
      "        }\n",
      "        i -= 2\n",
      "    \n",
      "\n",
      "ilt(block)\n",
      "\n",
      "c\n",
      "\n",
      "\n",
      "\n",
      "ilt(listOf(modules), context, moduleKind)\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "if (descriptor is CallableDescriptor && descriptor.overriddenDescriptors.isEmpty()) {\n",
      "    overriddenDescriptors.add(descriptor)\n",
      "} else {\n",
      "    val originalSignature = descriptor.originalSignature?: return null\n",
      "    if (originalSignature.isBadJavaSignature()) {\n",
      "        descriptor.overriddenDescriptors.add(overriddenDescriptors)\n",
      "    }\n",
      "}\n",
      "\n",
      "data.copy()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " {\n",
      "    return storage.lastIndexOf(element.toInt())\n",
      "}\n",
      "\n",
      "a.s = w(a)\n",
      "\n",
      "f()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      " {\n",
      "    if (2.toShort() in emptyCharSequence.indices!= range1.contains(2.toShort())) throw AssertionError()\n",
      "    if (2.toShort()!in emptyCharSequence.indices!=!range1.contains(2.toShort())) throw AssertionError()\n",
      "    if (!(2.toShort() in emptyCharSequence.indices)!=!range1.cont\n",
      "\n",
      "ilt(node, visitor)\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "remainder(v1.toLong(), v2.toLong()).toUInt()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iltocode<String>(compilerId, \"main\")\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "ilt(xNotNull())\n",
      "\n",
      "ilt(void)\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "  consume(block)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(from, to)\n",
      "\n",
      "\n",
      "\n",
      "(\"OK\").value()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ilt(x:?, y:?).let { ilt(x, y) }\n",
      "\n",
      "acos(x)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy score': 0.06, 'bleu score': 8.629398069945111e-06}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.evaluate(model, tokenizer, utils.CodeCompletionDataset(test_kotlin_dataset.prompts[:100], test_kotlin_dataset.answers[:100], train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy score': 0.04, 'bleu score': 0.05493027320889365}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "completions = []\n",
    "completion_tokens = []\n",
    "input_tokens = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, tokenizer, eval_dataset, *, max_new_tokens: int = 100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt, answer in tqdm(eval_dataset):\n",
    "            prompt_tensor = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            input_tokens.append(prompt_tensor)\n",
    "            output = model.generate(\n",
    "                prompt_tensor,\n",
    "                min_new_tokens=2,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                bos_token_id=tokenizer.bos_token_id,\n",
    "            )\n",
    "            completion_tokens.append(output[0])\n",
    "            completions.append(tokenizer.decode(output[0][len(prompt_tensor[0]):], skip_special_tokens=True))\n",
    "\n",
    "            answers.append(answer)\n",
    "    # for compl in completions:\n",
    "    #     print(compl)\n",
    "    #     print()\n",
    "\n",
    "    return {\n",
    "        \"accuracy score\": accuracy_score(answers, completions),\n",
    "        \"bleu score\": corpus_bleu([[ans.split()] for ans in answers], [comp.split() for comp in completions]),\n",
    "    }\n",
    "\n",
    "\n",
    "evaluate(model, tokenizer, utils.CodeCompletionDataset(test_kotlin_dataset.prompts[:100], test_kotlin_dataset.answers[:100], train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28534116965615375, 0.20281163341389366, 0.2819595708627902)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(completions, answers, avg=True)\n",
    "scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
